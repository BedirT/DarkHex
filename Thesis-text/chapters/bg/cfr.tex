\subsection{Counterfactual Regret Minimization (CFR)}

In this section, we will describe the Counterfactual Regret Minimization algorithm.
We will first start with terminology and definitions, continue with the regret in 
a simple non-sequential game of Rock Paper Scissors. We will then proceed and explain
the algorithm in its pure (vanilla) form before we go into variants such as 
MCCFR, RCFR, Deep CFR, and so on. We either used or tried to use the included 
variants in the experiments, therefore it is essential to understand the differences
between them. That's why we also included a section on the differences between these
variants, and which one is better to use in which situations, and why.
Throughout the section we will be using the earlier definitions from the
extensive form games and RL sections. 

\subsubsection{What is Regret and Regret Minimization} % (fold)
\label{sub:regret}
Consider yourself making an important decision; you will either study for your exam tomorrow
or go out with your friends who seem to be eager to hang out with you. Here
we could make a decision based on what we think is the best for us, and in the end, 
get a reward depending on our choice. We can intuitively think about what the regret will be. 
If we go out with friends, let's call it event $f$, we will get a reward $r_f$
and otherwise, if we study, let's call it event $s$, we get a reward $r_s$. Regret
will be the reward we got from the action chosen minus the reward we could get if we
have chosen the other action. In our case it will be -if we went out- $R = r_s - r_f$.
Simply how much do we \textbf{regret} not choosing the other action? 

Now let's add a third option, going home and watching a movie and call it $m$.
Now how do we calculate regret? The cumulative regret will follow from the first
part, and we sum the regrets we have from all simultaneous choices we 
didn't make;

$$R= \sum_{i\in E}{r_f-r_i}$$

where $E$ is the number of events we have and $r_f$ is the reward of the event we chose. 

The same logic goes for regret in games, every time we make a decision and take an action
we prioritize it over the others, which will let us calculate the cumulative regret.
We then can use this regret to optimize our actions if that situation was to occur again,
hence \textbf{minimizing the regret}.

\subsubsection{A Better Strategy} % (fold)
\label{ssub:betterStrategy}
We can use the notion of regret and self-simulated play to make our actions better 
in future states of the game. We will do this by minimizing expected regret in the 
game over time. Here we will consider the normal form zero-sum two-player simultaneous-move 
game of Rock-Paper-Scissors (RPS) because of its simplicity and being non-sequential 
(The round we are in, is not affecting the future rounds). This simple method will give us 
the Nash equilibrium if we run enough iterations.\\

The RPS game is a simple three-action game, each action has a counteraction, and an action it counters.
Rock beats Scissors, Scissors beats Paper, and Paper beats Rock. The players choose an action
simultaneously. Let the utility be the net gain/loss we make, assuming the players bet a dollar for
each round. The player who wins the round gains $+1$ while the other player gains $-1$ or loses $+1$.\\  

So in a simple turn that we play scissors and the opponent plays rock, our gain will be $-1$. We will be 
regretting not playing rock but regret even more for not playing paper. This is because our gain would
have been even higher if we did play paper. So the definition of the \textbf{regret} is the difference
between the utility of not choosing an action and the utility of choosing an action.\\

In more technical terms, let's call $a_i$ action player $i$ took, and $a_{-i}$ for all other players
actions, where $a \in \mathcal{A}$ is the action profile. Let's also call $u(a_i, a_{-i})$ the utility 
for player $i$ taking action $a_i$ and the other players taking actions $a_{-i}$. Let $a^{'}_i$ be an 
action in \mathcal{A} player $i$ did not take. Then the regret for picking $a_i$ over $a^{'}_i$ for player
$i$ is $u(a^{'}_i, a_{-i}) - u(a_i)$.\\

We can use this regret mechanism to evolve our future decisions and make them better. For that, we could
abuse our knowledge and emphasize the play on the move that brought us the least regret so far, i.e.
play scissors because you played rock for the last round and received a regret of 2 (opponent plays paper)
and now you don't want to regret so you should play scissors. This could work in a sense, but the opponent
might exploit our selections, so we do not want to be predictable. Here we have \textbf{regret matching}
to help us overcome this issue.\\

\textbf{Regret Matching:} Choosing actions randomly according to the positive regret distribution.
Positive regrets shows us the relative loss for not choosing an action. Therefor we use regret matching
and choose the next action proportional to the relative regret, for each action that will be the regret
over their sum. For example for picking scissors when opponent picked rock our relative regrets will be 
$0, \frac{1}{3}, \frac{2}{3}$ for scissors, rock and paper respectively.\\

If we show one more round, and let's say we play paper this time, and opponent plays scissors. We will
have regrets 0, 1, -1 respectively for scissors, rock and paper. Adding the positive regrets to the 
previous ones we now see that we have $0, \frac{2}{4}, \frac{2}{4}$ as a strategy.

% subsubsection  (end)

% subsection  (end)